# Testing {#scaling-testing}

```{r, include = FALSE}
source("common.R")
options(tibble.print_min = 6, tibble.print_max = 6)
```

For simple apps, its easy enough to remember how the app is supposed to work, so that when you make changes to add new features, you don't accidental break existing capabilities.
However, as your app gets more complicated it becomes impossible to hold it all in your head simultaneously.
Testing is a way to capture the desired behaviour of your code to work, and turn it into an automated tool that allows you verify that your code keeps working the way that you expect.
Turning your existing informal tests into code is going to be painful when you first do it (because you'll need to carefully turn every key press and mouse click into a line of code), but every time you need to run it, it is much much easier.

We'll perform automated testing with the [testthat](http://testthat.r-lib.org/) package.
A testthat test looks like this:

```{r, eval = FALSE}
test_that("as.vector() strips names", {
  x <- c(a = 1, b = 2)
  expect_equal(as.vector(x), c(1, 2))
})
```

We'll come back to the details very soon, but note that a test starts by declaring the intent ("`as.vector()` strips names) then uses regular R code to generate some test data.
The test data is then compared to the expected result using a **expectation**, a function that starts with `expect_`.
Here we verify that the output of `as.vector(x)` equals `c(1, 2)`.

We'll discuss four basic levels of testing in this chapter:

-   We'll start by testing **functions**.
    This will allow you to verify the behaviour of code that you've extracted out of the server function or UI, and help you learn the basic testing workflow.
    This is the exactly the same type of testing you'd do if you were writing a package, so you can find more details in the [testing chapter](https://r-pkgs.org/tests.html) of *R Packages*.

-   Next you'll learn how to test the flow of **reactivity** within your server function.
    You will simulate setting inputs to specific values then verifying that the reactives and outputs have the values you expect.

-   Then we'll test parts of Shiny that use **javascript** (most importantly, the `update*` family of functions) by running the app in a web browser.
    This is a higher fidelity simulation because it runs a real browser in the background, but the downside is that tests are slower to run, and you can no longer so easily peek inside the app to see the values of reactives.

-   Finally, we'll test **app visuals** by saving screenshots of selected elements.
    This is necessary for testing app layout and CSS, plots, and HTML widgets, but is fragile because screenshots can change for unrelated reasons.
    Human intervention is required to confirm whether each change is OK or not, making this the most labour intensive form of testing.

These levels of testing form a natural hierarchy because each technique provides a fuller simulation of the user experience of an app.
The downside of the better simulations is that each level gets progressively slower because it has to do more, and more fragile because more external forces come into play.
You should always strive to work at the lowest possible level so your tests are as fast and robust as possible.
Over time this will also influence the way you write code: knowing what sort of code is easier to test will naturally push you towards simpler designs.

Interleaved between the description of the levels of testing, I'll also provide advice about testing workflow and more general testing philosophy.

```{r setup, cache = FALSE}
library(shiny)
library(testthat)
library(shinytest)
```

This chapter uses features in the upcoming release of testthat 3.0.0 and shinytest 1.5.0, so if you're reading the book now you'll need to install their development versions from GitHub: `remotes::install_github(c("r-lib/testhat", "rstudio/shinytest"))`.

## Testing functions

The easiest part of your app is test is the part that has the least to do with Shiny: the functions that you've extracted out of your UI and server code as described in Chapter \@ref(scaling-functions).
We'll start by discussing how to test these non-reactive functions, showing you the basic structure of unit testing with the testthat package.

### Basic structure

Tests have three levels of hierarchy:

-   **File**.
    All test files live in `tests/testthat`.
    Each test file should correspond to a code file in `R/`, e.g.Â the code in `R/module.R` should be tested by the code in `tests/testthat/module.R`.
    Fortunately you don't have to remember that convention, you can just use `usethis::use_test()` to automatically create or locate the test file corresponding to the currently open R file.

-   **Test**.
    Each file is broken down into tests, a call to `test_that()`.
    A test should generally check a single property of a function.
    It's hard to describe exactly how to structure your tests, so I think the best you can do is practice.
    A good heuristic is that you can easily describe what the test does in the first argument to `test_that()`.

-   **Expectation**.
    Each test contains one or more expectations, with a functions that start with `expect_`.
    These are the lowest level These are very low level assertions.
    I'll discuss the most important expectations for Shiny apps here: `expect_equal()`, `expect_error()`, and `expect_snapshot_output()`.
    Many expectations others can be found on the [testthat website](https://testthat.r-lib.org/reference/index.html#section-expectations){.uri}.

The art of testing is figuring out how to write tests that clearly defines the expected behaviour of your function, without depending on incidental details that might change in the future.

### Basic workflow

Assume you've written `load_file()` from Section \@ref(function-upload):

```{r}
load_file <- function(name, path) {
  ext <- tools::file_ext(name)
  switch(ext,
    csv = vroom::vroom(path, delim = ","),
    tsv = vroom::vroom(path, delim = "\t"),
    validate("Invalid file; Please upload a .csv or .tsv file")
  )
}
```

And for the sake of this example it lives in `R/load.R`.
To test it, you first create a test file by calling `use_test()`, which creates `tests/testthat/load.R`.

Then we write a test.
There are three main things we want to test --- can it load a csv file, can it load a tsv file, and does it give an error message for other types?
To test that, I first have to create a little sample data, which I put in the temp directory so it's automatically cleaned up after my tests are run.
This is good practice because you want your tests to be as self-contained as possible.
Then I write three expectations, two checking that loaded file equals my original data, and one checking that I get an error.

```{r}
test_that("load_file() handles input types", {
  # Create sample data
  df <- tibble::tibble(x = 1, y = 2)
  path_csv <- tempfile()
  path_tsv <- tempfile()
  write.csv(df, path_csv, row.names = FALSE)
  write.table(df, path_tsv, sep = "\t", row.names = FALSE)
  
  expect_equal(load_file("test.csv", path_csv), df)
  expect_equal(load_file("test.tsv", path_tsv), df)
  expect_error(load_file("blah", path_csv), "Invalid file")
})
```

There are four ways to run this test:

-   As I'm developing it, I run each line interactively at the console.
    When an expectation fails, it turns into an error, which I then fix.

-   Once I've finished developing it, I run the whole test block.
    If the test passes, I get a message like `Test passed ðŸ˜€`.
    If it fails, I get the details of what went wrong.

-   As I develop more tests, I run all of the tests for the current file[^scaling-testing-1] with `devtools::test_file()`. Because I do this so often, I have a special keyboard shortcut set up to make it as easy as possible.
    I'll show you how to set that up yourself very shortly.

-   Every now and then I run all of the tests for the whole package with `devtools::test()`.
    This ensures that I haven't accidentally broken anything outside of the current file.

[^scaling-testing-1]: Like `usethis::use_test()` this only works if you're using RStudio.

### More server examples

What should your test contain?
How many tests per function?
Why?
When?

### Handling failures

When a test fails, you'll need to use your debugging skills to figure out why.

If you generally find it hard to debug a failing test, it may suggest that your tests are too complicated and you need to work on making them simpler; or that you need to deliberately practice your debugging skills.

### User interface functions

You can use the same basic idea to test functions that you've extracted out of your UI code.
But these require a new expectation, because manually typing out all the HTML would be tedious, so instead we use a snapshot test.
A snapshot expectation differs from other expectations primarily in that the expected result is stored in a separate snapshot file, rather than in the code itself.
Snapshot tests are most useful when you are designing complex user interface design systems, which is outside of the scope of most apps.
So here I'll briefly show you the key ideas, and then point you to additional resources to learn more.

Take this UI function we defined earlier:

```{r}
sliderInput01 <- function(id) {
  sliderInput(id, label = id, min = 0, max = 1, value = 0.5, step = 0.1)
}

as.character(sliderInput01("x"))
```

How would we test that this output is as we expect?
We could use `expect_equal()`:

```{r}
test_that("shinyInput01() creates expected HTML", {
  expect_equal(sliderInput("x"), "<div class=\"form-group shiny-input-container\">\n  <label class=\"control-label\" for=\"x\">x</label>\n  <input class=\"js-range-slider\" id=\"x\" data-min=\"0\" data-max=\"1\" data-from=\"0.5\" data-step=\"0.1\" data-grid=\"true\" data-grid-num=\"10\" data-grid-snap=\"false\" data-prettify-separator=\",\" data-prettify-enabled=\"true\" data-keyboard=\"true\" data-data-type=\"number\"/>\n</div>")
})
```

But the presence of quotes and newlines requires a lot of escaping in the string --- that makes it hard to see exactly what we expect, and if the output changes, makes it hard to see exactly what's happened.

The key idea of snapshot tests is to store the expected results in a separate file: that keeps bulky data out of your test code, and means that you don't need to worry about escaping special values in a string.
Here we use `expect_snapshot_output()` to capture the output displayed on the console:

```{r}
test_that("shinyInput01() creates expected HTML", {
  expect_snapshot_output(sliderInput01("x"))
})
```

The main difference with other expectations is that there's no second argument that describes what you expect to see.
Instead, that data is saved in separate file named by convention: assuming that your code is in `R/slider.R` and your test is in `tests/testthat/test-slider.R`, then snapshot will be saved in `tests/testhat/_snaps/slider.md`.
The first time you run the test, `expect_snapshot_output()` will automatically create the reference output, which will look like this:

``` {.md}
# shinyInput01() creates expected HTML

    <div class="form-group shiny-input-container">
      <label class="control-label" for="x">x</label>
      <input class="js-range-slider" id="x" data-min="0" data-max="1" data-from="0.5" data-step="0.1" data-grid="true" data-grid-num="10" data-grid-snap="false" data-prettify-separator="," data-prettify-enabled="true" data-keyboard="true" data-data-type="number"/>
    </div>
```

If the output later changes, the test will fail.
You either need to fix the bug that causes it to fail, or if it's a deliberate change, update the snapshot by running `testthat::snapshot_accept()`.

It's worth contemplating the output here before committing to this as a test.
What are you really testing here?
If you look at how the inputs become the outputs you'll notice that most of the output is generated by Shiny and only a very small amount is the result of your code.
That suggests this test isn't particularly useful: if this output changes, it's much more likely to be the result of change to Shiny than the result of a change to your code.
This makes the test fragile; if it fails it's unlikely to be your fault, and fixing the failure is unlikely to be within your control.

You can learn more about snapshot tests at <https://testthat.r-lib.org/articles/snapshotting.html>.

## Workflow

Before we talk about reactivity and javascript, we'll take a brief digression to work on your workflow before diving into .

### Code coverage

`devtools::test_coverage()` and `devtools::test_coverage_file()` will perform "code coverage", running all the tests and recording which lines of code are run.
This is useful to check that you have tested the lines of code that you think you have tested, and gives you an opportunity to reflect on if you've tested the most important, highest risk, or hardest to program parts of your code.

Won't cover in detail here, but I highly recommend trying it out.
Main thing to notice is that green lines are tested; red lines are not.

Basic workflow: Write tests.
Inspect coverage.
Contemplate why lines were tested.
Add more tests.
Repeat.

Not a substitute for thinking about corner cases --- you can have 100% test coverage and still have bugs.
But it's a fun and a useful tool to help you think about what's important, particularly when you have complex nested code.

### Keyboard shortcuts

If you followed the advice in Section \@ref(package-workflow) then you can already run tests just by typing `test()` or `test_file()` at the console.
But tests are something that you'll do so often it's worth having a keyboard shortcut at your finger tips.

RStudio has one useful shortcut built in: Cmd/Ctrl + Shift + T run `devtools::test()`.
I recommend that you add three yourself to complete the set:

-   Cmd/Ctrl + T to `devtools::test_file()`

-   Cmd/Ctrl + Shift + R to `devtools::test_coverage()`

-   Cmd/Ctrl + R to `devtools::test_coverage_file()`

You're of course free to choose whatever shortcut makes sense to you, but these have share some underlying structure.
Keyboard shortcuts using Shift apply to the whole package, and without shift apply to the current file.

This is what my keyboard shortcuts look like for the mac.

```{r, echo = FALSE, out.width = NULL}
knitr::include_graphics("screenshots/scaling-testing/keyboard-shortcuts.png", dpi = 300)
```

### Summary

-   From the R file, use `usethis::use_test()` to create the test file (the first time its run) or navigate to the test file (if it already exists).

-   Write code/write tests.
    Press `cmd/ctrl + T` to run the tests and review the results in the console.
    Iterate as needed.

-   If you encounter a new bug, start by capturing the bad behaviour in a test.
    In the course of making the minimal code, you'll often get a better understanding of where the bug lies, and having the test will ensure that you can't fool yourself into thinking that you've fixed the bug when you haven't.

-   Press `ctrl/cmd + R` to check that you're testing what you think you're testing

-   Press `ctrl/cmd + shift + T` to make you have accidentally broken anything else.

## Testing reactivity

Now that you've tested your non-reactive code, it's time to move on to challenges specific to Shiny.
The first challenge is testing reactivity.
As you've already discovered, you can't normally run reactive code interactively:

```{r, eval = FALSE}
x <- reactive(input$y + input$z)
x()
```

Not only does the reactive error when we attempt to evaluate it, even if it did work `input$y` and `input$z` wouldn't be defined.
To test reactive code, we need a new technique that only appeared in Shiny 1.5.0.
To see how it works, let's start with a simple app that has three inputs, one output, and three reactives:

```{r}
ui <- fluidPage(
  numericInput("x", "x", 0),
  numericInput("y", "y", 1),
  numericInput("z", "z", 2),
  textOutput("out")
)
server <- function(input, output, session) {
  xy <- reactive(input$x - input$y)
  yz <- reactive(input$z + input$y)
  xyz <- reactive(xy() * yz())
  output$out <- renderText(paste0("Result: ", xyz()))
}
myApp <- function(...) {
  shinyApp(ui, server, ...)
}
```

We can't test with code using the techniques above because all the complexity is in the reactivity inside the server function.
So we need a new technique powered by `testServer()`.
This function takes two arguments: an app and code to run as if it was inside the server function.
The code is run in a special environment that allows you to access the output values, reactives, and a special `session` that allows you to simulate user interaction.
The main time you'll use this is for `session$setInputs()` which allows you to set the value of input controls, as if you were a user interacting with the app in a browser.

```{r}
testServer(myApp(), {
  session$setInputs(x = 1, y = 1, z = 1)
  print(xy())
  print(output$out)
})
```

(If you want to interactively experiment with the reactivity, you can use `browser()`: `testServer(myApp(), browser())`)

Despite the fact that we're passing a complete app to `testServer()`, we are only testing the server function; the `ui` component of the app is effectively ignored.
You can see this most clearly by inspecting the inputs: unlike a real Shiny app, all inputs start as `NULL`, because the initial value is recorded in the `ui`.
We'll come back to UI testing in Section \@ref(testing-javascript).

```{r}
testServer(myApp(), {
  print(input$x)
})
```

That's because this is a pure server side simulation; while we give it that app object that contains both the UI and server, it only uses the server function.

Now that you have a way to run code inside the server function, you can combine it with what you already know about testing code, to create something like this:

```{r}
test_that("reactives and output updates", {
  testServer(myApp(), {
    session$setInputs(x = 1, y = 1, z = 1)
    expect_equal(xy(), 0)
    expect_equal(yz(), 2)
    expect_equal(output$out, "Result: 0")
  })
})
```

### Modules

You can test a module in a similar way to testing an app function (but again remember you're just testing the server side of things; `testServer()` always ignores the ui).
Let's start with a simple module that uses three outputs to display a brief summary of a variable:

```{r}
summaryUI <- function(id) {
  tagList(
    outputText(ns(id, "min")),
    outputText(ns(id, "mean")),
    outputText(ns(id, "max")),
  )
}
summaryServer <- function(id, var) {
  stopifnot(is.reactive(var))
  
  moduleServer(id, function(input, output, session) {
    range_val <- reactive(range(var(), na.rm = TRUE))
    output$min <- renderText(range_val()[[1]])
    output$max <- renderText(range_val()[[2]])
    output$mean <- renderText(mean(var()))
  })
}
```

We'll use `testServer()` as above, but the call is a little different.
This time, the first argument is the module server, and then we supply the arguments to that function.
Note that the `id` argument is always automatically filled in.
Here we need to give it a reactive value.
Then we finish up with the code to run:

```{r}
x <- reactiveVal(1:10)
testServer(summaryServer, args = list(var = x), {
  print(range_val())
  print(output$min)
})
```

Here I wrap it all up into a test that checks that the module responds correctly as the reactive value changes:

```{r, cache = FALSE}
test_that("output updates when reactive input changes", {
  x <- reactiveVal()
  testServer(summaryServer, args = list(var = x), {
    x(1:10)
    expect_equal(range_val(), c(1, 10))
    
    x(10:20)
    expect_equal(range_val(), c(10, 20))
  }) 
})
```

If your module has a return value (a reactive or list of reactives), you can capture it with `session$getReturned()`.
Then you can check the value of that reactive, just like any other reactive.

```{r}
datasetServer <- function(id) {
  moduleServer(id, function(input, output, session) {
    reactive(get(input$dataset, "package:datasets"))
  })
}

test_that("can find dataset", {
  testServer(datasetServer, {
    dataset <- session$getReturned()
    
    session$setInputs(dataset = "mtcars")
    expect_equal(dataset(), mtcars)
    
    session$setInputs(dataset = "iris")
    expect_equal(dataset(), iris)
  })
})
```

Do we need to test what happens if `input$dataset` isn't a dataset?
In this case, no because we know that the module UI restricts the options to valid choices.
That's not obvious from inspection of the server function alone.

### Timers

### Limitations

`testServer()` is a simulation of your app.
The simulation is useful because it lets you quickly test reactive code, but it is not complete.

-   Unlike the real world, time does not advance automatically.
    So if you want to best code that relies on `reactiveTimer()` or `invalidateLater()`, you'll need to manually advance time by calling `session$elapse(millis = 300)`.

-   `testServer()` ignores the UI.
    That means inputs don't get default values, and no javascript works.
    We'll come back to this very shortly, but this means that you can't currently test the update functions, because they work by sending JS to the browser that simulates the user making a change.
    You'll require the next technique to test such code.

## Philosophy

### When should you write tests?

When should you write tests?
There are three basic options

-   **Before you write the code**.
    This is a style of code called test driven development, and if you know exactly how a function should behave, it makes sense to capture that knowledge as code *before* you start writing the implementation.

-   **After you write the code**.
    While writing code you'll often build up a mental to-do list of worries about your code.
    After you've written the function, turn these into tests so that you can be confident that the function works the way that you expect.

    When you start writing tests, beware writing them too soon.
    If your function is still actively evolving, keeping your tests up to date with all the changes is going to feel frustrating.
    That may indicate you need to wait a little longer.

-   **When you find a bug**.
    Whenever you find a bug, it's good practice to turn it into an automated test case.
    This has two advantages.
    Firstly, to make a good test case, you'll need to relentlessly simplify the problem until you have a very minimal reprex that you can include in a test.
    Secondly, you'll make sure that the bug never comes back again!

## Testing javascript

Because `testServer()` runs only a limited simulation of the full Shiny app, no Shiny function that uses javascript will work.
You might wonder why that's a problem, because we haven't talked at all about using custom javascript in your app.
However, a number of important shiny functions use javascript behind the scenes.
These include:

-   Any `update*()` function, Section \@ref(updating-inputs).

-   `showNotification()`/`removeNotification()`, Section \@ref(notifications).

-   `showModal()`/`hideModal()`, Section \@ref(feedback-modal).

-   `insertUI()`/`removeUI()`/`appendTab()`/`insertTab()`/`removeTab()`, which we'll cover later in the book.

To test these functions you need to run the Shiny app in a real browser which actually runs javascript.
We can do this with an off-label use of the [shinytest](https://rstudio.github.io/shinytest) package.
You can use it as the website recommends, automatically generating test code using an app, but I think it yields tests that are a little more fragile than desirable.

What does headless mean?

The main advantage of using shinytest is that it's very high fidelity because it actually starts up an R process and a browser in the background, and then controls them using code.
The primary downside is that this is slower than the other approaches (it takes at least a second for even the simplest apps), and you can only the outside of the app (i.e. you can't see the values of specific reactives, only their outcomes on the app itself).

### Basic operation

To use shinytest you start an app with `app <- ShinyDriver$new()`, interact with it using `app$setInputs()` and friends, then test values returned by `app$getValue()`:

```{r, eval = FALSE}
test_that("app works", {
  app <- shinytest::ShinyDriver$new(myapp())
  app$setInputs(x = 1)
  expect_equal(app$getValue("y"), 2)
})
```

The code chunk above show the three most important methods of ShinyDriver that you'll use all the time:

-   `ShinyDriver$new()` takes either a Shiny app object, or a path to a Shiny app.
    Typically when testing a package, you'll use the former.
    This call takes a couple of seconds to run because it has to launch a new R process to run your Shiny app, and a new headless browser to simulate someone using your app.

-   `app$setInputs()` works much like `session$setInputs()` above --- it updates the values in the browser, and then waits until all reactive updates are complete.

-   `app$getValue()` retrieves the value of an output control.

See `?ShinyDriver` for more details, and a list of more esoteric methods.

### Case study

To explore this we'll start with the radio button + other example from Section \@ref(radio-other):

```{r}
ui <- fluidPage(
  radioButtons("extra", "Select one",
    choiceNames = list(
      "apple", 
      "pear", 
      textInput("other", label = NULL, placeholder = "Other")
    ),
    choiceValues = c("apple", "pear", "other")
  ), 
  textOutput("value")
)

server <- function(input, output, session) {
  observeEvent(input$other, ignoreInit = TRUE, {
    updateRadioButtons(session, "extra", selected = "other")
  })
    
  output$value <- renderText({
    if (input$extra == "other") {
      req(input$other)
      input$other
    } else {
      input$extra
    }
  })
}
```

We'll start by testing everything we can with `testServer()`:

```{r}
test_that("returns other value when primary is other", {
  testServer(shinyApp(ui, server), {
    session$setInputs(extra = "apple")
    expect_equal(output$value, "apple")
    
    session$setInputs(extra = "other", other = "orange")
    expect_equal(output$value, "orange")
  })  
})
```

That doesn't check that other is automatically selected when we start typing in the other box.
We can't test that using `testServer()` because it relies on `updateRadioButtons()`:

```{r, error = TRUE}
test_that("returns other value when primary is other", {
  testServer(shinyApp(ui, server), {
    session$setInputs(other = "orange")
    expect_equal(output$value, "orange")
  })  
})
```

So now we need to use ShinyDriver:

```{r, eval = FALSE}
test_that("automatically switches to other", {
  app <- ShinyDriver$new(shinyApp(ui, server))
  app$setInput(primary = "apple")
  expect_equal(app$getValue("value"), "apple")
  
  app$setInput(other = "orange")
  expect_equal(app$getValue("primary"), "other")
  expect_equal(app$getValue("value"), "orange")
})
```

### Modules

If this was a module, as in the original Section \@ref(radio-other), then there's an additional complexity --- how do set inputs inside a module?
That's usually forbidden by code due to namespacing.
But now we need some way to refer to those controls/

### Advanced controls

For more advanced interactions with input and output controls, you'll need to use `control <- app$findWidget("name")` to get a Widget object.
This object gives additional additional controls:

-   `control$click()` simulates a mouse click on the control.

-   `control$sendKeys(keys)` simulates individual key presses.

There are also a couple of methods that are specialised for specific control types:

-   `control$uploadFile(path)` simulates uploading a file to a `fileInput()`

-   `control$listTabs()` lists the tabs on a `tabsetPanel()`.

Note that when you move away from `app$setInputs()` it starts to become your responsibility to check that the app has completed reactive updates before retrieving values using `getValues()`.

## Testing visuals

What about components like plots or HTML widgets where it's difficult to describe the correct appearance using code?
You can use the final, richest, and most fragile testing technique: save a screenshot of the affected component.
This combines screenshotting from shinytest with whole-file snapshotting from testthat.
It works similarly to the snapshotting described in Section \@ref(user-interface-functions) but there's one important difference: it's no longer possible to see the differences on the console.
So instead you'll be prompted to run `testthat::snapshot_review()` which uses a Shiny app to visually show the differences.

```{r}
expect_snapshot_screenshot <- function(app, id, name, parent = FALSE) {
  skip_on_ci()

  path <- tempfile()
  app$screenshot(path, id, parent = parent)
  expect_snapshot_file(path, name)
}
```

The primary downside of testing using screenshots is that even the tiniest of changes requires a human confirm that it's OK.
This is a problem because it's hard to get different computers to generate pixel-reproducible screenshots.
Differences in operating system, browser version, and even font versions, can lead to screenshots that look the same to a human, but are very slightly different.
This generally means that visual tests are best run by one person on their local computer, and it's generally not worthwhile to run them in a continuous integration tool.
It is possible to work around these issues, but it's considerable challenge and beyond the scope of this book.
